@incollection{qian2019automatic,
author = {Qian, Yao and Lange, Patrick and Evanini, Keelan},
booktitle = {Automated Speaking Assessment: Using Language Technologies to Score Spontaneous Speech},
pages = {61--72},
publisher = {Routledge},
title = {{Automatic Speech Recognition for Automated Speech Scoring}},
year = {2019}
}
@inproceedings{Ramanarayanan2018,
author = {Ramanarayanan, Vikram and Pautler, David and Lange, Patrick and Tsuprun, Eugene and Ubale, Rutuja and Evanini, Keelan and Suendermann-Oeft, David},
booktitle = {Proc. of Interspeech, 19th Annual Conference of the International Speech Communication Association},
pages = {1960--1961},
title = {{Toward Scalable Dialog Technology for Conversational Language Learning: Case Study of the TOEFL{\textregistered} MOOC.}},
year = {2018}
}
@inproceedings{Pautler2018,
address = {Stroudsburg, PA, USA},
author = {Pautler, David and Ramanarayanan, Vikram and Cofino, Kirby and Lange, Patrick and Suendermann-Oeft, David},
booktitle = {Proc of SIGdial, 19th Annual Meeting of the the Special Interest Group on Discourse and Dialogue},
doi = {10.18653/v1/W18-5029},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Pautler et al. - 2018 - Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher T.pdf:pdf},
pages = {249--252},
publisher = {Association for Computational Linguistics},
title = {{Leveraging Multimodal Dialog Technology for the Design of Automated and Interactive Student Agents for Teacher Training}},
url = {http://aclweb.org/anthology/W18-5029},
year = {2018}
}
@inproceedings{Loukina2019,
abstract = {Use of speech technologies in the classroom is often limited by the inferior acoustic conditions as well as other factors that might affect the quality of the recordings. We describe My-TurnToRead, an e-book-based app designed to support an inter-leaved listening and reading experience, where the child takes turns reading aloud with a virtual partner. The child's reading turns are recorded, and processed by an automated speech analysis system in order to provide feedback or track improvement in reading skill. We describe the architecture of the speech processing back-end and evaluate system performance on the data collected in several summer camps where children used the app on consumer-grade devices as part of the camp programming. We show that while the quality of the audio recordings varies greatly, our estimates of student oral reading fluency are very good: for example, the correlation between ASR-based and transcription-based estimates of reading fluency at the speaker level is r=0.93. These are also highly correlated with an external measure of reading comprehension.},
author = {Loukina, Anastassia and Klebanov, Beata Beigman and Lange, Patrick and Qian, Yao and Gyawali, Binod and Madnani, Nitin and Misra, Abhinav and Zechner, Klaus and Wang, Zuowei and Sabatini, John},
booktitle = {Proc. of Interspeech, 20th Annual Conference of the International Speech Communication Association},
doi = {10.21437/Interspeech.2019-2889},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Loukina et al. - 2019 - Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead.pdf:pdf},
keywords = {[Electronic Manuscript]},
pages = {21--25},
title = {{Automated Estimation of Oral Reading Fluency During Summer Camp e-Book Reading with MyTurnToRead}},
url = {http://dx.doi.org/10.21437/Interspeech.2019-2889},
year = {2019}
}
@incollection{Yu2017a,
abstract = {{\textcopyright} 2019, Springer International Publishing AG, part of Springer Nature. In complex conversation tasks, people react to their interlocutor's state, such as uncertainty and engagement to improve conversation effectiveness Forbes-Riley and Litman (Adapting to student uncertainty improves tutoring dialogues, pp 33–40, 2009 [2]). If a conversational system reacts to a user's state, would that lead to a better conversation experience? To test this hypothesis, we designed and implemented a dialog system that tracks and reacts to a user's state, such as engagement, in real time. We designed and implemented a conversational job interview task based on the proposed framework. The system acts as an interviewer and reacts to user's disengagement in real-time with positive feedback strategies designed to re-engage the user in the job interview process. Experiments suggest that users speak more while interacting with the engagement-coordinated version of the system as compared to a non-coordinated version. Users also reported the former system as being more engaging and providing a better user experience.},
address = {Farmington, USA},
author = {Yu, Zhou and Ramanarayanan, Vikram and Lange, Patrick and Suendermann-Oeft, David and Suendermann‐Oeft, David},
booktitle = {Advanced Social Interaction with Agents},
doi = {10.1007/978-3-319-92108-2_21},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2019 - An Open-Source Dialog System with Real-Time Engagement Tracking for Job Interview Training Applications.pdf:pdf},
isbn = {9783319921075},
issn = {18761119},
keywords = {Automated interviewing,Engagement,Multimodal dialog systems},
pages = {199--207},
publisher = {Springer, Cham},
title = {{An Open-Source Dialog System with Real-Time Engagement Tracking for Job Interview Training Applications}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:aqlVkmm33-oC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2019}
}
@inproceedings{Qian2017a,
abstract = {Copyright {\textcopyright} 2017 ISCA. Identifying a speaker's native language with his speech in a second language is useful for many human-machine voice interface applications. In this paper, we use a sub-phone-based i-vector approach to identify non-native English speakers' native languages by their English speech input. Time delay deep neural networks (TDNN) are trained on LVCSR corpora for improving the alignment of speech utterances with their corresponding sub-phonemic "senone" sequences. The phonetic variability caused by a speaker's native language can be better modeled with the sub-phone models than the conventional phone model based approach. Experimental results on the database released for the 2016 Interspeech ComParE Native Language challenge with 11 different L1s show that our system outperforms the best system by a large margin (87.2{\%} UAR compared to 81.3{\%} UAR for the best system from the 2016 ComParE challenge).},
address = {Stockholm, Sweden},
author = {Qian, Yao and Evanini, Keelan and Wang, Xinhao and Suendermann-Oeft, David and Pugh, Robert A. R.A. and Lange, Patrick and Molloy, Hillary R H.R. Hillary and Soong, F.K. Frank K.},
booktitle = {Proc. of Interspeech, 18th Annual Conference of the International Speech Communication Association},
doi = {10.21437/Interspeech.2017-245},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Qian et al. - 2017 - Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech.pdf:pdf},
issn = {19909772},
keywords = {I-vector,Native language identification,Time delay deep neural network},
pages = {2586--2590},
title = {{Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:9ZlFYXVOiuMC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
volume = {2017-Augus},
year = {2017}
}
@inproceedings{Madnani2019,
abstract = {Literacy is crucial for functioning in modern society. It underpins everything from educational attainment and employment opportunities to health outcomes. We describe My Turn To Read, an app that uses interleaved reading to help developing and struggling readers improve reading skills while reading for meaning and pleasure. We hypothesize that the longer-term impact of the app will be to help users become better, more confident readers with an increased stamina for extended reading. We describe the technology and present preliminary evidence in support of this hypothesis.},
author = {Madnani, Nitin and Klebanov, Beata Beigman and Loukina, Anastassia and Gyawali, Binod and Sabatini, John and Lange, Patrick and Flor, Michael and Sabatini, John and Flor, Michael},
booktitle = {Proc. of ACL, 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Madnani et al. - 2019 - My Turn To Read An Interleaved E-book Reading Tool for Developing and Struggling Readers.pdf:pdf},
pages = {141--146},
title = {{My Turn To Read: An Interleaved E-book Reading Tool for Developing and Struggling Readers}},
url = {https://www.nationsreportcard.gov/reading{\_}},
year = {2019}
}
@inproceedings{Evanini2018,
abstract = {{\textcopyright} 2018 International Speech Communication Association. All rights reserved. This demo presents three different spoken dialog applications that were developed to provide young learners of English an opportunity to practice speaking and to receive feedback on particular aspects of their English speaking ability. The speaking tasks were designed as game-based interactions in order to engage young students, and they provide feedback about grammar (yes/no question formation and simple past tense verb formation) and vocabulary. A pilot study with 27 primary-level English as a foreign language (EFL) learners investigated the usefulness of these applications.},
author = {Evanini, Keelan and Timpe-Laughlin, Veronika and Tsuprun, Eugene and Blood, Ian and Lee, Jeremy and Bruno, James V and Ramanarayanan, Vikram and Lange, Patrick and Suendermann-Oeft, David},
booktitle = {Proc. of INTERSPEECH, 19th Annual Conference of the International Speech Communication Association},
doi = {10.21437/Interspeech.2018-3045},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Evanini et al. - 2018 - Game-based spoken dialog language learning applications for young students.pdf:pdf},
issn = {19909772},
keywords = {Grammar feedback,Language learning,SDS applications},
pages = {548--549},
title = {{Game-based spoken dialog language learning applications for young students}},
volume = {2018-Septe},
year = {2018}
}
@inproceedings{Qian2017b,
abstract = {{\textcopyright} 2017 IEEE. Spoken language understanding (SLU) in dialog systems is generally performed using a natural language understanding (NLU) model based on the hypotheses produced by an automatic speech recognition (ASR) system. However, when new spoken dialog applications are built from scratch in real user environments that often have sub-optimal audio characteristics, ASR performance can suffer due to factors such as the paucity of training data or a mismatch between the training and test data. To address this issue, this paper proposes an ASR-free, end-to-end (E2E) modeling approach to SLU for a cloud-based, modular spoken dialog system (SDS). We evaluate the effectiveness of our approach on crowdsourced data collected from non-native English speakers interacting with a conversational language learning application. Experimental results show that our approach is particularly promising in situations with low ASR accuracy. It can further improve the performance of a sophisticated CNN-based SLU system with more accurate ASR hypotheses by fusing the scores from E2E system, i.e., the overall accuracy of SLU is improved from 85.6{\%} to 86.5{\%}.},
author = {Qian, Yao and Ubale, Rutuja and Ramanaryanan, Vikram and Lange, Patrick and Suendermann-Oeft, David and Evanini, Keelan and Tsuprun, Eugene},
booktitle = {Proc. of ASRU, 15th IEEE Automatic Speech Recognition and Understanding Workshop},
doi = {10.1109/ASRU.2017.8268987},
isbn = {9781509047888},
keywords = {end-to-end,spoken language understanding},
pages = {569--576},
title = {{Exploring ASR-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system}},
year = {2017}
}
@inproceedings{Qian2019,
abstract = {{\textcopyright} 2019 IEEE. We present Neural Network (NN) approaches to the automated assessment of non-native spontaneous speech in a monologic task and a simulated dialogic task. Three attention-based Bidirectional Long Short-Term Memory (BLSTM) Recurrent Neural Networks (RNN) are employed to learn three dimensions (i.e., delivery, language use, and content) of scoring rubrics for the spoken responses. The prompts or turn history information are encoded to low-dimensional vectors by either a BLSTM-RNN or an end-to-end memory network (MemN2N) and used as the conditions of the inputs of the NN for rating the subscore of content. The three subscores are fused together to generate a holistic score. The experimental results show that our approaches significantly outperform the conventional approaches to speech scoring and the correlations of automatically predicted scores with the reference human scores are higher than human-human agreement levels for both tasks.},
author = {Qian, Yao and Lange, Patrick and Evanini, Keelan and Pugh, Robert and Ubale, Rutuja and Mulholland, Matthew and Wang, Xinhao},
booktitle = {Proc. of ICASSP, 44th IEEE International Conference on Acoustics, Speech and Signal Processing},
doi = {10.1109/ICASSP.2019.8683717},
isbn = {9781479981311},
issn = {15206149},
keywords = {LSTM,RNN,attention,automated speech scoring,end-to-end memory networks},
pages = {8112--8116},
title = {{Neural Approaches to Automated Speech Scoring of Monologue and Dialogue Responses}},
volume = {2019-May},
year = {2019}
}
@inproceedings{Cofino2017,
abstract = {{\textcopyright} 2017 ACM. We present an open-source multimodal dialog system equipped with a virtual human avatar interlocutor. The agent, rigged in Blender and developed in Unity with WebGL support, interfaces with the HALEF open-source cloud-based standard-compliant dialog framework. To demonstrate the capabilities of the system, we designed and implemented a conversational job interview scenario where the avatar plays the role of an interviewer and responds to user input in real-Time to provide an immersive user experience.},
author = {Cofino, Kirby and Ramanarayanan, Vikram and Lange, Patrick and Pautler, David and Suendermann-Oeft, David and Evanini, Keelan},
booktitle = {Proc. of ICMI, 19th ACM International Conference on Multimodal Interaction},
doi = {10.1145/3136755.3143034},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Cofino et al. - 2017 - A modular, multimodal open-source virtual interviewer dialog agent.pdf:pdf},
isbn = {9781450355438},
keywords = {Avatar,Dialog system,Multimodal,Open-source,Virtual agent},
pages = {520--521},
title = {{A modular, multimodal open-source virtual interviewer dialog agent}},
volume = {2017-Janua},
year = {2017}
}
@inproceedings{Qian2017c,
abstract = {{\textcopyright} 2017 IEEE. Identifying a speaker's native language (L1), i.e., mother tongue, based upon non-native English (L2) speech input, is both challenging and useful for many human-machine voice interface applications, e.g., computer assisted language learning (CALL). In this paper, we improve our sub-phone TDNN based i-vector approach to L1 recognition with a more accurate TDNN-derived VAD and a highly discriminative classifier. Two TDNNs are separately trained on native and non-native English, LVCSR corpora, for contrasting their corresponding sub-phone posteriors and resultant supervectors. The derived i-vectors are then exploited for improving the performance further. Experimental results on a database of 25 L1s show a 3.1{\%} identification rate improvement, from 78.7{\%} to 81.8{\%}, compared with a high performance baseline system which has already achieved the best published results on the 2016 ComParE corpus of only 11 L1s. The statistical analysis of the features used in our system provides useful findings, e.g. pronunciation similarity among the non-native English speakers with different L1s, for research on second-language (L2) learning and assessment.},
author = {Qian, Yao and Evanini, Keelan and Lange, Patrick and Pugh, Robert and Ubale, Rutuja and Soong, Frank K.},
booktitle = {Proc. of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
doi = {10.1109/ASRU.2017.8268992},
file = {:C$\backslash$:/Users/patri/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2017 - Improving native language (L1) identifation with better VAD and TDNN trained separately on native and non-native En.pdf:pdf},
isbn = {9781509047888},
keywords = {i-vector,native language identification,time delay deep neural networks (TDNN)},
pages = {606----613},
title = {{Improving native language (L1) identifation with better VAD and TDNN trained separately on native and non-native English corpora}},
year = {2017}
}
@inproceedings{Qian2018,
abstract = {� 2018 IEEE Spoken language understanding (SLU) is to decode the semantic information embedded in speech input. SLU decoding can be significantly degraded by mismatched acoustic/language models between training and testing of a decoder. In this paper we investigate the semantic tagging performance of bidirectional LSTM RNN (BLSTM-RNN) with input at acoustic, phonetic and word levels. It is tested on a crowdsourced, spoken dialog speech corpus spoken by non-native speakers in a job interview task. The tagging performance is shown to be improved successively from low-level, acoustic MFCC, mid-level, stochastic senone posteriorgram, to high-level, ASR recognized word string, with the corresponding tagging accuracies at 70.6{\%}, 82.1{\%} and 85.1{\%}, respectively. With a score fusion of the three individual RNNs together, the accuracy can be further improved to 87.0{\%}.},
author = {Qian, Yao and Ubale, Rutuja and Lange, Patrick and Evanini, Keelan and Soong, Frank},
booktitle = {Proc. of ISCSLP, 11th International Symposium on Chinese Spoken Language Processing},
doi = {10.1109/ISCSLP.2018.8706581},
isbn = {9781538656273},
keywords = {MFCC,Posteriorgram,Senone,Spoken language understanding},
pages = {280--284},
title = {{From Speech Signals to Semantics—Tagging Performance at Acoustic, Phonetic and Word Levels}},
year = {2018}
}
@inproceedings{Loukina2017,
author = {Loukina, Anastassia and {Beigman Klebanov}, Beata and Lange, Patrick and Gyawali, Binod and Qian, Yao},
booktitle = {Proc. of WOCCI, 6th international workshop on child computer interaction},
doi = {10.21437/WOCCI.2017-8},
file = {:Users/plange/Projects/private/langep.github.io/assets/pdf/Loukina2017.pdf:pdf},
number = {November},
pages = {46--51},
title = {{Developing speech processing technologies for shared book reading with a computer}},
url = {http://www.isca-speech.org/archive/WOCCI{\_}2017/abstracts/WOCCI{\_}2017{\_}paper{\_}9.html},
year = {2017}
}
@article{Ramanarayanan2017,
author = {Ramanarayanan, Vikram and Lange, Patrick and Evanini, Keelan and Molloy, Hillary and Tsuprun, Eugene and Qian, Yao and Suendermann-Oeft, David},
doi = {10.1002/ets2.12146},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2017 - Using Vision and Speech Features for Automated Prediction of Performance Metrics in Multimodal Dialogs(2).pdf:pdf},
journal = {ETS Research Report Series},
month = {apr},
number = {1},
pages = {1--11},
publisher = {Wiley Periodicals, Inc.},
title = {{Using Vision and Speech Features for Automated Prediction of Performance Metrics in Multimodal Dialogs}},
url = {http://doi.wiley.com/10.1002/ets2.12146},
volume = {2017},
year = {2017}
}
@article{Suendermann-Oeft2017,
author = {Suendermann-Oeft, David and Ramanarayanan, Vikram and Yu, Zhou and Qian, Yao and Evanini, Keelan and Lange, Patrick and Wang, Xinhao and Zechner, Klaus},
doi = {10.1002/ets2.12149},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Suendermann-Oeft et al. - 2017 - A Multimodal Dialog System for Language Assessment Current State and Future Directions.pdf:pdf},
journal = {ETS Research Report Series},
month = {may},
number = {1},
pages = {1--7},
publisher = {Wiley Periodicals, Inc.},
title = {{A Multimodal Dialog System for Language Assessment: Current State and Future Directions}},
url = {http://doi.wiley.com/10.1002/ets2.12149},
volume = {2017},
year = {2017}
}
@article{Ramanarayanan2016,
author = {Ramanarayanan, Vikram and Suendermann-Oeft, David and Lange, Patrick and Ivanov, Alexei V. and Evanini, Keelan and Yu, Zhou and Tsuprun, Eugene and Qian, Yao},
doi = {10.1002/ets2.12105},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2016 - Bootstrapping Development of a Cloud-Based Spoken Dialog System in the Educational Domain From Scratch Usi.pdf:pdf},
journal = {ETS Research Report Series},
month = {jun},
number = {1},
pages = {1--7},
publisher = {Wiley Periodicals, Inc.},
title = {{Bootstrapping Development of a Cloud-Based Spoken Dialog System in the Educational Domain From Scratch Using Crowdsourced Data}},
url = {http://doi.wiley.com/10.1002/ets2.12105},
volume = {2016},
year = {2016}
}
@inproceedings{Mehrez2013,
address = {Darmstadt, Germany},
author = {Mehrez, Tarek and Abdelkawy, Abdelrahman and Heikal, Youmna and Lange, Patrick and Nabil, Hadeer and Suendermann-Oeft, David},
booktitle = {Proc. of GSCL, International Conference of the German Society for Computational Linguistics and Language Technology},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Mehrez et al. - 2013 - Who Discovered the Electron Neutrino A Telephony-Based Distributed Open-Source Standard-Compliant Spoken Dialog S.pdf:pdf},
title = {{Who Discovered the Electron Neutrino? A Telephony-Based Distributed Open-Source Standard-Compliant Spoken Dialog System for Question Answering}},
url = {http://www.halef.org/su/pdf/gscl2013a.pdf},
volume = {80},
year = {2013}
}
@inproceedings{Lange2014,
address = {Dresden, Germany},
author = {Lange, Patrick and Suendermann-Oeft, David},
booktitle = {Proc. of ESSV, Conference on Electronic Speech Signal Processing},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Lange, Suendermann-Oeft - 2014 - Tuning Sphinx to Outperform Google's Speech Recognition API.pdf:pdf},
pages = {32 --41},
title = {{Tuning Sphinx to Outperform Google's Speech Recognition API}},
url = {http://antikenschlacht.com/su/pdf/essv2014.pdf},
year = {2014}
}
@inproceedings{Ivanov2016,
address = {Saariselk, Finland},
author = {Ivanov, Alexei V. and Lange, Patrick and Suendermann-Oeft, David and Ramanarayanan, Vikram and Qian, Yao and Yu, Zhou and Tao, Jidong},
booktitle = {Proc. of IWSDS, International Workshop on Spoken Dialog Systems},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ivanov et al. - 2016 - Speed vs. Accuracy Designing an Optimal ASR System for Spontaneous Non-Native Speech in a Real-Time Application.pdf:pdf},
title = {{Speed vs. Accuracy: Designing an Optimal ASR System for Spontaneous Non-Native Speech in a Real-Time Application}},
url = {http://www.oeft.de/su/pdf/iwsds2016.pdf},
year = {2016}
}
@inproceedings{Ivanov2016a,
address = {Los Angeles, USA},
author = {Ivanov, Alexei V. and Lange, Patrick and Suendermann-Oeft, David},
booktitle = {Proc. of SIGdial, 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ivanov, Lange, Suendermann-Oeft - 2016 - LVCSR System on a Hybrid GPU-CPU Embedded Platform for Real-Time Dialog Applications.pdf:pdf},
isbn = {978-1-945626-23-4},
pages = {220--223},
title = {{LVCSR System on a Hybrid GPU-CPU Embedded Platform for Real-Time Dialog Applications}},
url = {https://www.aclweb.org/anthology/W/W16/W16-36.pdf{\#}page=238},
year = {2016}
}
@inproceedings{Ivanov2015,
address = {Scottsdale, USA},
author = {Ivanov, Alexei V. and Lange, Patrick and Suendermann-Oeft, David},
booktitle = {Proc. of ASRU, 14th IEEE Automatic Speech Recognition and Understanding Workshop},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ivanov, Lange, Suendermann-Oeft - 2015 - Fast and Power Efficient Hardware-Accelerated Cloud-Based ASR for Remote Dialog Applications.pdf:pdf},
pages = {220 -- 223},
title = {{Fast and Power Efficient Hardware-Accelerated Cloud-Based ASR for Remote Dialog Applications}},
url = {http://ehrai.com/su/pdf/asru2015a.pdf},
year = {2015}
}
@inproceedings{Mory2014,
address = {Venice, Italy},
author = {Mory, Martin and Lange, Patrick and Mehrez, Tarek and Suendermann-Oeft, David},
booktitle = {Proc. of NLPCS, Natural Language Processing and Cognitive Science},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Mory et al. - 2014 - Evaluation of Freely Available Speech Synthesis Voices for Halef.pdf:pdf},
pages = {207--213},
publisher = {Walter de Gruyter GmbH {\&} Co KG},
title = {{Evaluation of Freely Available Speech Synthesis Voices for Halef}},
url = {http://www.oeft.com/su/pdf/nlpcs2014a.pdf},
year = {2014}
}
@inproceedings{Ramanarayanan2016a,
address = {Las Vegas, USA},
author = {Ramanarayanan, Vikram and Suendermann-Oeft, David and Lange, Patrick and Mundkowsky, Robert and Ivanov, Alexei V. and Yu, Zhou and Qian, Yao and Evanini, Keelan},
booktitle = {Proc. of CMLA, Workshop on Computational Models for Learning Systems and Educational Assessment},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2016 - Development of an Audiovisual Database of Human-Machine Conversations for Educational Learning and Assessm.pdf:pdf},
title = {{Development of an Audiovisual Database of Human-Machine Conversations for Educational Learning and Assessment Applications}},
url = {http://www.cs.ucf.edu/{~}smkhan/CMLA2016/papers/Ramanayaran{\_}development-audiovisual-database-6May2016.pdf},
year = {2016}
}
@inproceedings{Ramanarayanan2015,
address = {Scottsdale, USA},
author = {Ramanarayanan, Vikram and Yu, Zhou and Mundkowsky, Robert and Lange, Patrick and Ivanov, Alexei V. and Black, Alan W. and Suendermann‐Oeft, David and Suendermann-Oeft, David},
booktitle = {Proc. of ASRU, 14th IEEE Automatic Speech Recognition and Understanding Workshop},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2015 - A Modular Open-Source Standard-Compliant Dialog System Framework with Video Support.pdf:pdf},
title = {{A Modular Open-Source Standard-Compliant Dialog System Framework with Video Support}},
url = {https://pdfs.semanticscholar.org/8c3d/7f33debb4359e65c9d8cfe6113e166910ba1.pdf},
year = {2015}
}
@incollection{Yu2017,
author = {Yu, Zhou and Ramanarayanan, Vikram and Mundkowsky, Robert and Lange, Patrick and Ivanov, Alexei V. and Black, Alan W. and Suendermann-Oeft, David},
booktitle = {Dialogues with Social Robots},
doi = {10.1007/978-981-10-2585-3_18},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2017 - Multimodal HALEF An Open-Source Modular Web-Based Multimodal Dialog Framework.pdf:pdf},
pages = {233--244},
publisher = {Springer, Singapore},
title = {{Multimodal HALEF: An Open-Source Modular Web-Based Multimodal Dialog Framework}},
url = {http://link.springer.com/10.1007/978-981-10-2585-3{\_}18},
year = {2017}
}
@incollection{Ramanarayanan2017a,
author = {Ramanarayanan, Vikram and Suendermann-Oeft, David and Lange, Patrick and Mundkowsky, Robert and Ivanov, Alexei V. and Yu, Zhou and Qian, Yao and Evanini, Keelan},
booktitle = {Multimodal Interaction with W3C Standards},
editor = {Dahl, Deborah},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2017 - Assembling the jigsaw How multiple open standards are synergistically combined in the HALEF multimodal dia.pdf:pdf},
isbn = {978-3-319-42814-7},
pages = {295--310},
publisher = {Springer, Cham},
title = {{Assembling the jigsaw: How multiple open standards are synergistically combined in the HALEF multimodal dialog system}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:ULOm3{\_}A8WrAC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2017}
}
@inproceedings{Ramanarayanan2016b,
address = {San Diego, USA},
author = {Ramanarayanan, Vikram and Lange, Patrick and Pautler, David and Yu, Zhou and Suendermann-Oeft, David and {Intention Perception}, L L C and Suendermann‐Oeft, David},
booktitle = {Proc. of SLT, IEEE Workshop on Spoken Language Technology},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2016 - Interview with an Avatar A Real-Time Engagement Tracking-Enabled Cloud-Based Multimodal Dialog System for.pdf:pdf},
title = {{Interview with an Avatar: A Real-Time Engagement Tracking-Enabled Cloud-Based Multimodal Dialog System for Learning and Assessment}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:YOwf2qJgpHMC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2016}
}
@inproceedings{Ramanarayanan2017b,
address = {San Francisco, USA},
author = {Ramanarayanan, Vikram and Suendermann-Oeft, David and Molloy, Hillary and Tsuprun, Eugene and Lange, Patrick and Evanini, Keelan},
booktitle = {Proc. of the Workshop on Crowdsourcing, Deep Learning and Artificial Intelligence Agents at the Thirty-First AAAI Conference on Artificial Intelligence},
file = {:Users/plange/Library/Application Support/Mendeley Desktop/Downloaded/Ramanarayanan et al. - 2017 - Crowdsourcing Multimodal Dialog Interactions Lessons Learned from the HALEF Case.pdf:pdf},
title = {{Crowdsourcing Multimodal Dialog Interactions: Lessons Learned from the HALEF Case}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:4TOpqqG69KYC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2017}
}
@inproceedings{Qian2017,
address = {Saarbr{\"{u}}cken, Germany},
author = {Qian, Yao and Ubale, Rutuja and Ramanaryanan, Vikram and Lange, Patrick and Suendermann-Oeft, David and Evanini, Keelan and Tsuprun, Eugene and Ramanarayanan, Vikram and Lange, Patrick and Suendermann‐Oeft, David and Evanini, Keelan and Tsuprun, Eugene},
booktitle = {Proc. pf SEMDAIL, 21st Workshop on the Semantics and Pragmatics of Dialogue},
pages = {160--161},
title = {{Towards End-to-End Modeling of Spoken Language Understanding in a Cloud-based Spoken Dialog System}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:4DMP91E08xMC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2017}
}
@inproceedings{Schnelle-Walka2017,
address = {Saarbr{\"{u}}cken, Germany},
author = {Schnelle-Walka, Dirk and Radomski, Stefan and Ramanarayanan, Vikram and Lange, Patrick and Suendermann-Oeft, David and Radomski, Stefan and Lange, Patrick and Suendermann‐Oeft, David},
booktitle = {Proc. of SEMDIAL, 21st Workshop on the Semantics and Pragmatics of Dialogue},
pages = {164--165},
title = {{An Open Source Standards-Compliant Voice Browser with Support for Multiple Language Understanding Implementations}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:Wp0gIr-vW9MC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2017}
}
@inproceedings{Ramanarayanan2017c,
address = {Stockholm, Sweden},
author = {Ramanarayanan, Vikram and Lange, Patrick and Evanini, Keelan and Molloy, Hillary and Suendermann-Oeft, David},
booktitle = {Proc. of Interspeech, 18th Annual Conference of the International Speech Communication Association},
doi = {10.21437/Interspeech.2017-1213},
pages = {1711--1715},
title = {{Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human – Machine Spoken Dialog Interactions}},
url = {/citations?view{\_}op=view{\_}citation{\&}continue=/scholar{\%}3Fhl{\%}3Den{\%}26as{\_}sdt{\%}3D0,5{\%}26scilib{\%}3D1{\%}26scioq{\%}3DComparing{\%}2Bopen-source{\%}2Bspeech{\%}2Brecognition{\%}2Btoolkits{\&}citilm=1{\&}citation{\_}for{\_}view=6TxeqJwAAAAJ:mVmsd5A6BfQC{\&}hl=en{\&}scioq=Comparing+open-source+speech+reco},
year = {2017}
}